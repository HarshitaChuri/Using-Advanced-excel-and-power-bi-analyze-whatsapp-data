# -*- coding: utf-8 -*-
"""PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FpYOxnB14bKmn3f6d_O-ROUGUBhQh89l
"""

from google.colab import drive
drive.mount('/content/drive')

# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv("/content/drive/MyDrive/DATASCIENCE/study_performance.csv")

df.head()

# Check for missing values
print(df.isnull().sum())

df.info()

# Encode categorical variables
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Display the first few rows after encoding
print(df.head())

"""EDA"""

print(df.describe())

corr_matrix = df.corr()
print(corr_matrix)

# Heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

# Pairplot to visualize relationships
sns.pairplot(df)
plt.show()

# Count plots for categorical features
plt.figure(figsize=(12, 8))
for i, column in enumerate(['gender', 'race_ethnicity', 'parental_level_of_education', 'lunch', 'test_preparation_course']):
    plt.subplot(2, 3, i+1)
    sns.countplot(data=df, x=column)
    plt.title(f'Count Plot of {column.capitalize()}')
plt.tight_layout()
plt.show()

# Box plots for score distributions
plt.figure(figsize=(12, 8))
for i, column in enumerate(['math_score', 'reading_score', 'writing_score']):
    plt.subplot(1, 3, i+1)
    sns.boxplot(data=df, y=column)
    plt.title(f'Box Plot of {column.replace("_", " ").capitalize()}')
plt.tight_layout()
plt.show()

"""LINEAR REGRESSION"""

X = df[['reading_score', 'writing_score']]
y = df['math_score']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predictions
y_pred = linear_model.predict(X_test)

# Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f'Mean Absolute Error: {mae}')
print(f'Root Mean Squared Error: {rmse}')

# Visualization of predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.xlabel('Actual Math Score')
plt.ylabel('Predicted Math Score')
plt.title('Actual vs Predicted Math Score')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()

"""LOGISTIC REGRESSION"""

# Logistic Regression to predict whether a student scored above average in math
df['high_math_score'] = (df['math_score'] > df['math_score'].median()).astype(int)
X = df[['reading_score', 'writing_score']]
y = df['high_math_score']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

# Predictions
y_pred = logistic_model.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

# Visualization of confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""INTERPRETAION OF RESULTS

Linear Regression Results
Mean Absolute Error (MAE): 7.351

Interpretation: On average, the predicted math scores are off by about 7.35 points from the actual scores. This indicates the average magnitude of the errors in the predictions.
Root Mean Squared Error (RMSE): 8.789

Interpretation: The RMSE value is about 8.79, which gives an idea of the standard deviation of the prediction errors. RMSE is more sensitive to large errors compared to MAE.
Conclusion:

The linear regression model's predictions have an average error of around 7.35 points and a typical error magnitude of about 8.79 points.

Logistic Regression Results
Accuracy: 0.82

Interpretation: The model correctly classified 82% of the instances in the test set. Accuracy measures the overall correctness of the model.
Confusion Matrix:

True Negatives (TN): 83

False Positives (FP): 21

False Negatives (FN): 15

True Positives (TP): 81


Interpretation: The model correctly predicted 83 instances as low math scores and 81 instances as high math scores. It incorrectly predicted 21 instances as high math scores (false positives) and 15 instances as low math scores (false negatives).


Classification Report:

Precision: The proportion of positive identifications that were actually correct.

Class 0 (Low): 0.85

Class 1 (High): 0.79

Recall: The proportion of actual positives that were correctly identified.

Class 0 (Low): 0.80

Class 1 (High): 0.84

F1-score: The harmonic mean of precision and recall.

Class 0 (Low): 0.82

Class 1 (High): 0.82



Interpretation:
The model has a balanced precision and recall for both classes, indicating it performs well in identifying both high and low math scores.
Precision and recall are slightly higher for class 0 (low scores), indicating the model is slightly better at identifying low scores.


Conclusion:

The logistic regression model has good performance with an accuracy of 82%.

The model shows balanced precision and recall, with slight variation between the two classes.

The confusion matrix indicates that the model has a reasonable number of false positives and false negatives, suggesting it is reliable in identifying high and low math scores.
"""